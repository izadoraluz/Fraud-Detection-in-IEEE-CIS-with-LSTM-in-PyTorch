{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Detecção de Fraudes no IEEE-CIS Fraud Detection com LSTM no PyTorch**\n",
        "\n",
        "**Objective:** Optimize a pre-trained neural network model for credit card fraud detection. Apply advanced hyperparameter fine-tuning techniques, such as grid search and random search, to improve model performance metrics, including precision, recall, F1-score, and AUC-ROC. The activity also requires a comparison between the optimized model and the original model, allowing the assessment of the impact of hyperparameter modifications on overall performance.\n",
        "\n",
        "**TLDR;**\n",
        "\n",
        "The pipeline was sound from an engineering standpoint (temporal splitting, scaler, callbacks), but the global sequence hypothesis doesn't hold for this dataset—and the class is extremely rare. The final result (AUC ≈ 0.52; very low F1) indicates that the LSTM, as the sequences were formed, didn't learn a useful signal. Restructuring the problem by entity or migrating to a tabular model with good features should yield large and immediate gains; then, adjust the threshold and costs to achieve the desired recall without causing false positives."
      ],
      "metadata": {
        "id": "mPZA5KjWbkez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importações\n",
        "\n"
      ],
      "metadata": {
        "id": "_k-iI40nNaTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gdown\n",
        "import gdown\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "ewQolAMWMeqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f192f9c-c89d-4c58-a171-9ad302ee84e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurações"
      ],
      "metadata": {
        "id": "xXdB2FWUNfMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arquivo_destino_colab = \"dataset.csv\"\n",
        "doc_id = \"1u_OWAPkIdgJw1ah5xP_dGBFMSANxjxEl\"\n",
        "URL = f\"https://drive.google.com/uc?id={doc_id}\"\n",
        "gdown.download(URL, arquivo_destino_colab, quiet=False)\n",
        "\n",
        "\n",
        "SAVE_FIGS = False\n",
        "LOOKBACK = 10\n",
        "VAL_FRACTION = 0.1\n",
        "TEST_FRACTION = 0.2\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 1e-3\n",
        "NEGATIVE_TO_POS_RATIO_TRAIN = 10\n",
        "SEED = 42\n",
        "\n",
        "# Fixar sementes para reprodutibilidade\n",
        "def set_seeds(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "# Pasta para figuras\n",
        "if SAVE_FIGS:\n",
        "    os.makedirs(\"figs\", exist_ok=True)"
      ],
      "metadata": {
        "id": "kAlRxjfkMezc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "e46a16fe-2450-459a-ec0d-497e5bfb103c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1u_OWAPkIdgJw1ah5xP_dGBFMSANxjxEl\n",
            "From (redirected): https://drive.google.com/uc?id=1u_OWAPkIdgJw1ah5xP_dGBFMSANxjxEl&confirm=t&uuid=dcddb62d-0abe-458d-9168-3e42b6dff18c\n",
            "To: /content/dataset.csv\n",
            "100%|██████████| 151M/151M [00:02<00:00, 61.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dataset.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "C6PofvsYKjLG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "gVh-tEcsODe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_log1p(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Aplica log1p em valores não negativos; se houver negativos, desloca.\"\"\"\n",
        "    if (series < 0).any():\n",
        "        shift = abs(series.min()) + 1e-9\n",
        "        return np.log1p(series + shift)\n",
        "    return np.log1p(series)\n",
        "\n",
        "def print_header(title: str):\n",
        "    print(\"\\n\" + \"=\"*len(title))\n",
        "    print(title)\n",
        "    print(\"=\"*len(title))\n",
        "\n",
        "def describe_correlation_with_class(df: pd.DataFrame, feature_cols, target_col=\"Class\", top_k=10):\n",
        "    \"\"\"Correlação ponto-biserial (Pearson com rótulo binário) por feature.\"\"\"\n",
        "    corrs = []\n",
        "    for c in feature_cols:\n",
        "        try:\n",
        "            r, p = pearsonr(df[c].values, df[target_col].values)\n",
        "            corrs.append((c, r, p))\n",
        "        except Exception:\n",
        "            corrs.append((c, np.nan, np.nan))\n",
        "    corr_df = pd.DataFrame(corrs, columns=[\"feature\", \"r_point_biserial\", \"p_value\"]).sort_values(\n",
        "        by=\"r_point_biserial\", key=lambda s: s.abs(), ascending=False\n",
        "    )\n",
        "    print_header(\"Top correlações (|r|) com Class\")\n",
        "    print(corr_df.head(top_k).to_string(index=False))\n",
        "    return corr_df\n",
        "\n",
        "def chronological_split(df: pd.DataFrame, time_col=\"Time\", test_fraction=0.2):\n",
        "    \"\"\"Divide cronologicamente: parte mais recente vira teste.\"\"\"\n",
        "    df_sorted = df.sort_values(time_col).reset_index(drop=True)\n",
        "    split_idx = int(len(df_sorted) * (1 - test_fraction))\n",
        "    train_val_df = df_sorted.iloc[:split_idx].reset_index(drop=True)\n",
        "    test_df = df_sorted.iloc[split_idx:].reset_index(drop=True)\n",
        "    return train_val_df, test_df\n",
        "\n",
        "def build_sequences(X: np.ndarray, y: np.ndarray, lookback: int):\n",
        "    \"\"\"Cria sequências deslizantes X_seq (n_samples, lookback, n_feat) e targets y_seq (n_samples,).\"\"\"\n",
        "    if lookback < 1:\n",
        "        raise ValueError(\"lookback deve ser >= 1\")\n",
        "    Xs, ys = [], []\n",
        "    for i in range(lookback, len(X)):\n",
        "        Xs.append(X[i - lookback:i, :])\n",
        "        ys.append(y[i])\n",
        "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.int32)\n",
        "\n",
        "def undersample_negatives(X, y, max_ratio=10, seed=42):\n",
        "    \"\"\"Mantém todos os positivos e reduz negativos para ~max_ratio:1 (no conjunto de TREINO).\"\"\"\n",
        "    pos_idx = np.where(y == 1)[0]\n",
        "    neg_idx = np.where(y == 0)[0]\n",
        "    if len(pos_idx) == 0:\n",
        "        return X, y  # nada a fazer\n",
        "    max_neg = min(len(neg_idx), max_ratio * len(pos_idx))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    selected_neg = rng.choice(neg_idx, size=max_neg, replace=False) if len(neg_idx) > max_neg else neg_idx\n",
        "    keep = np.concatenate([pos_idx, selected_neg])\n",
        "    rng.shuffle(keep)\n",
        "    return X[keep], y[keep]\n",
        "\n",
        "def best_threshold_from_validation(y_true_val, y_score_val):\n",
        "    \"\"\"Escolhe threshold que maximiza F1 na validação.\"\"\"\n",
        "    thresholds = np.linspace(0.05, 0.95, 19)\n",
        "    best_t = 0.5\n",
        "    best_f1 = -1\n",
        "    for t in thresholds:\n",
        "        y_pred = (y_score_val >= t).astype(int)\n",
        "        f1 = f1_score(y_true_val, y_pred, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "    return best_t, best_f1\n",
        "\n",
        "def build_lstm_model(n_features: int, lookback: int, lr=1e-3) -> tf.keras.Model:\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=(lookback, n_features)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            tf.keras.metrics.AUC(curve=\"ROC\", name=\"auc\"),\n",
        "            tf.keras.metrics.Precision(name=\"precision\"),\n",
        "            tf.keras.metrics.Recall(name=\"recall\")\n",
        "        ]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "1J-HsXYROFIn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA - Análise Exploratória\n",
        "\n"
      ],
      "metadata": {
        "id": "QuA-H9fnNjZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_header(\"Visão geral\")\n",
        "print(df.head(5))\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "print_header(\"Valores ausentes por coluna\")\n",
        "print(df.isna().sum().sort_values(ascending=False))\n",
        "\n",
        "print_header(\"Estatísticas descritivas (numéricas)\")\n",
        "print(df.describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]).T)\n",
        "\n",
        "# Distribuição das classes (desbalanceamento)\n",
        "print_header(\"Distribuição de Classes\")\n",
        "class_counts = df[\"Class\"].value_counts().sort_index()\n",
        "class_pct = 100 * class_counts / len(df)\n",
        "print(pd.DataFrame({\"count\": class_counts, \"pct_%\": class_pct.round(4)}))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4,3))\n",
        "sns.countplot(x=\"Class\", data=df, ax=ax)\n",
        "ax.set_title(\"Distribuição da variável alvo (Class)\")\n",
        "plt.close(fig)\n",
        "\n",
        "# Distribuições de Amount (linear e log)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10,3))\n",
        "sns.histplot(df[\"Amount\"], bins=100, ax=axes[0])\n",
        "axes[0].set_title(\"Amount (linear)\")\n",
        "\n",
        "sns.histplot(safe_log1p(df[\"Amount\"]), bins=100, ax=axes[1])\n",
        "axes[1].set_title(\"log1p(Amount)\")\n",
        "plt.close(fig)\n",
        "\n",
        "# Amount por classe\n",
        "fig, ax = plt.subplots(figsize=(5,3))\n",
        "sns.boxplot(x=\"Class\", y=\"Amount\", data=df, ax=ax, showfliers=True)\n",
        "ax.set_title(\"Amount por Class (outliers visíveis)\")\n",
        "plt.close(fig)\n",
        "\n",
        "# Correlação entre features e Class (ponto-biserial)\n",
        "feature_cols = [c for c in df.columns if c not in [\"Class\"]]\n",
        "corr_df = describe_correlation_with_class(df, feature_cols, target_col=\"Class\", top_k=12)\n",
        "\n",
        "# Heatmap de correlação entre features (Pearson)\n",
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "corr_mat = df.drop(columns=[\"Class\"]).corr()\n",
        "sns.heatmap(corr_mat, cmap=\"vlag\", center=0, ax=ax)\n",
        "ax.set_title(\"Matriz de correlações (features)\")\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_8RPCbtMq5j",
        "outputId": "3436acd7-ea50-467c-c834-33f4be012265"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===========\n",
            "Visão geral\n",
            "===========\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n",
            "\n",
            "===========================\n",
            "Valores ausentes por coluna\n",
            "===========================\n",
            "Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n",
            "\n",
            "====================================\n",
            "Estatísticas descritivas (numéricas)\n",
            "====================================\n",
            "           count          mean           std         min           1%  \\\n",
            "Time    284807.0  9.481386e+04  47488.145955    0.000000  2422.000000   \n",
            "V1      284807.0  1.168375e-15      1.958696  -56.407510    -6.563199   \n",
            "V2      284807.0  3.416908e-16      1.651309  -72.715728    -4.960300   \n",
            "V3      284807.0 -1.379537e-15      1.516255  -48.325589    -3.978377   \n",
            "V4      284807.0  2.074095e-15      1.415869   -5.683171    -3.122987   \n",
            "V5      284807.0  9.604066e-16      1.380247 -113.743307    -3.060396   \n",
            "V6      284807.0  1.487313e-15      1.332271  -26.160506    -2.124023   \n",
            "V7      284807.0 -5.556467e-16      1.237094  -43.557242    -3.012847   \n",
            "V8      284807.0  1.213481e-16      1.194353  -73.216718    -4.033899   \n",
            "V9      284807.0 -2.406331e-15      1.098632  -13.434066    -2.455288   \n",
            "V10     284807.0  2.239053e-15      1.088850  -24.588262    -2.092670   \n",
            "V11     284807.0  1.673327e-15      1.020713   -4.797473    -2.093086   \n",
            "V12     284807.0 -1.247012e-15      0.999201  -18.683715    -3.063938   \n",
            "V13     284807.0  8.190001e-16      0.995274   -5.791881    -2.283101   \n",
            "V14     284807.0  1.207294e-15      0.958596  -19.214325    -2.799763   \n",
            "V15     284807.0  4.887456e-15      0.915316   -4.498945    -2.383923   \n",
            "V16     284807.0  1.437716e-15      0.876253  -14.129855    -2.381823   \n",
            "V17     284807.0 -3.772171e-16      0.849337  -25.162799    -1.349962   \n",
            "V18     284807.0  9.564149e-16      0.838176   -9.498746    -2.147067   \n",
            "V19     284807.0  1.039917e-15      0.814041   -7.213527    -2.075516   \n",
            "V20     284807.0  6.406204e-16      0.770925  -54.497720    -1.568406   \n",
            "V21     284807.0  1.654067e-16      0.734524  -34.830382    -1.469679   \n",
            "V22     284807.0 -3.568593e-16      0.725702  -10.933144    -1.654625   \n",
            "V23     284807.0  2.578648e-16      0.624460  -44.807735    -1.193417   \n",
            "V24     284807.0  4.473266e-15      0.605647   -2.836627    -1.657308   \n",
            "V25     284807.0  5.340915e-16      0.521278  -10.295397    -1.420859   \n",
            "V26     284807.0  1.683437e-15      0.482227   -2.604551    -1.009384   \n",
            "V27     284807.0 -3.660091e-16      0.403632  -22.565679    -1.247746   \n",
            "V28     284807.0 -1.227390e-16      0.330083  -15.430084    -0.876265   \n",
            "Amount  284807.0  8.834962e+01    250.120109    0.000000     0.120000   \n",
            "Class   284807.0  1.727486e-03      0.041527    0.000000     0.000000   \n",
            "\n",
            "                  5%           25%           50%            75%  \\\n",
            "Time    25297.600000  54201.500000  84692.000000  139320.500000   \n",
            "V1         -2.899147     -0.920373      0.018109       1.315642   \n",
            "V2         -1.971975     -0.598550      0.065486       0.803724   \n",
            "V3         -2.389740     -0.890365      0.179846       1.027196   \n",
            "V4         -2.195683     -0.848640     -0.019847       0.743341   \n",
            "V5         -1.702021     -0.691597     -0.054336       0.611926   \n",
            "V6         -1.406757     -0.768296     -0.274187       0.398565   \n",
            "V7         -1.434423     -0.554076      0.040103       0.570436   \n",
            "V8         -0.842147     -0.208630      0.022358       0.327346   \n",
            "V9         -1.758426     -0.643098     -0.051429       0.597139   \n",
            "V10        -1.338636     -0.535426     -0.092917       0.453923   \n",
            "V11        -1.571901     -0.762494     -0.032757       0.739593   \n",
            "V12        -1.967162     -0.405571      0.140033       0.618238   \n",
            "V13        -1.639729     -0.648539     -0.013568       0.662505   \n",
            "V14        -1.439351     -0.425574      0.050601       0.493150   \n",
            "V15        -1.593200     -0.582884      0.048072       0.648821   \n",
            "V16        -1.491663     -0.468037      0.066413       0.523296   \n",
            "V17        -0.983004     -0.483748     -0.065676       0.399675   \n",
            "V18        -1.358094     -0.498850     -0.003636       0.500807   \n",
            "V19        -1.356259     -0.456299      0.003735       0.458949   \n",
            "V20        -0.558435     -0.211721     -0.062481       0.133041   \n",
            "V21        -0.504674     -0.228395     -0.029450       0.186377   \n",
            "V22        -1.081892     -0.542350      0.006782       0.528554   \n",
            "V23        -0.472246     -0.161846     -0.011193       0.147642   \n",
            "V24        -1.143662     -0.354586      0.040976       0.439527   \n",
            "V25        -0.825026     -0.317145      0.016594       0.350716   \n",
            "V26        -0.697348     -0.326984     -0.052139       0.240952   \n",
            "V27        -0.415246     -0.070840      0.001342       0.091045   \n",
            "V28        -0.317843     -0.052960      0.011244       0.078280   \n",
            "Amount      0.920000      5.600000     22.000000      77.165000   \n",
            "Class       0.000000      0.000000      0.000000       0.000000   \n",
            "\n",
            "                  95%            99%            max  \n",
            "Time    164143.400000  170560.940000  172792.000000  \n",
            "V1           2.081223       2.237130       2.454930  \n",
            "V2           1.808585       3.801811      22.057729  \n",
            "V3           2.062635       2.728434       9.382558  \n",
            "V4           2.566501       4.248032      16.875344  \n",
            "V5           2.098960       3.424903      34.801666  \n",
            "V6           3.160382       4.200085      73.301626  \n",
            "V7           1.407632       2.696205     120.589494  \n",
            "V8           1.049984       2.075973      20.007208  \n",
            "V9           1.780783       2.986773      15.594995  \n",
            "V10          1.548557       3.253618      23.745136  \n",
            "V11          1.614033       2.290583      12.018913  \n",
            "V12          1.243053       1.698576       7.848392  \n",
            "V13          1.607877       2.513962       7.126883  \n",
            "V14          1.393653       2.150300      10.526766  \n",
            "V15          1.373090       1.925527       8.877742  \n",
            "V16          1.325253       1.874820      17.315112  \n",
            "V17          1.274609       2.289928       9.253526  \n",
            "V18          1.394392       2.068689       5.041069  \n",
            "V19          1.286164       2.262924       5.591971  \n",
            "V20          0.836144       2.412190      39.420904  \n",
            "V21          0.537868       1.931852      27.202839  \n",
            "V22          1.128987       1.530152      10.503090  \n",
            "V23          0.488016       1.508703      22.528412  \n",
            "V24          0.866358       1.063748       4.584549  \n",
            "V25          0.760699       1.203955       7.519589  \n",
            "V26          0.920915       1.158698       3.517346  \n",
            "V27          0.387746       0.931360      31.612198  \n",
            "V28          0.256090       0.541126      33.847808  \n",
            "Amount     365.000000    1017.970000   25691.160000  \n",
            "Class        0.000000       0.000000       1.000000  \n",
            "\n",
            "=======================\n",
            "Distribuição de Classes\n",
            "=======================\n",
            "        count    pct_%\n",
            "Class                 \n",
            "0      284315  99.8273\n",
            "1         492   0.1727\n",
            "\n",
            "===============================\n",
            "Top correlações (|r|) com Class\n",
            "===============================\n",
            "feature  r_point_biserial  p_value\n",
            "    V17         -0.326481      0.0\n",
            "    V14         -0.302544      0.0\n",
            "    V12         -0.260593      0.0\n",
            "    V10         -0.216883      0.0\n",
            "    V16         -0.196539      0.0\n",
            "     V3         -0.192961      0.0\n",
            "     V7         -0.187257      0.0\n",
            "    V11          0.154876      0.0\n",
            "     V4          0.133447      0.0\n",
            "    V18         -0.111485      0.0\n",
            "     V1         -0.101347      0.0\n",
            "     V9         -0.097733      0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparação dos dados para LSTM\n"
      ],
      "metadata": {
        "id": "33GVeCHvOOZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_header(\"Preparação dos dados para LSTM (split temporal, escalonamento, janelas)\")\n",
        "\n",
        "# Ordena por tempo e divide TREINO/TESTE cronologicamente (evita vazamento futuro->passado)\n",
        "train_val_df, test_df = chronological_split(df, time_col=\"Time\", test_fraction=TEST_FRACTION)\n",
        "print(f\"Train+Val shape: {train_val_df.shape} | Test shape: {test_df.shape}\")\n",
        "\n",
        "# Dentro do TREINO, separamos uma validação temporal (parte final do treino)\n",
        "split_idx_tv = int(len(train_val_df) * (1 - VAL_FRACTION))\n",
        "train_df = train_val_df.iloc[:split_idx_tv].reset_index(drop=True)\n",
        "val_df = train_val_df.iloc[split_idx_tv:].reset_index(drop=True)\n",
        "\n",
        "print(f\"Train shape: {train_df.shape} | Val shape: {val_df.shape} | Test shape: {test_df.shape}\")\n",
        "\n",
        "# Trata nulos (se houver). Aqui: imputação simples com mediana.\n",
        "for col in feature_cols:\n",
        "    med = train_df[col].median()\n",
        "    train_df[col] = train_df[col].fillna(med)\n",
        "    val_df[col]   = val_df[col].fillna(med)\n",
        "    test_df[col]  = test_df[col].fillna(med)\n",
        "\n",
        "# Escalonamento (fit no TREINO, aplica em VAL/TEST) — StandardScaler é ok para PCA e Amount\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_df[feature_cols].values)\n",
        "\n",
        "X_train_all = scaler.transform(train_df[feature_cols].values)\n",
        "y_train_all = train_df[\"Class\"].values.astype(int)\n",
        "\n",
        "X_val_all = scaler.transform(val_df[feature_cols].values)\n",
        "y_val_all = val_df[\"Class\"].values.astype(int)\n",
        "\n",
        "X_test_all = scaler.transform(test_df[feature_cols].values)\n",
        "y_test_all = test_df[\"Class\"].values.astype(int)\n",
        "\n",
        "# Criação de sequências (deslizante, etiqueta = último passo)\n",
        "X_train_seq, y_train_seq = build_sequences(X_train_all, y_train_all, LOOKBACK)\n",
        "X_val_seq,   y_val_seq   = build_sequences(X_val_all, y_val_all, LOOKBACK)\n",
        "X_test_seq,  y_test_seq  = build_sequences(X_test_all, y_test_all, LOOKBACK)\n",
        "\n",
        "print(f\"Seq shapes -> X_train: {X_train_seq.shape}, X_val: {X_val_seq.shape}, X_test: {X_test_seq.shape}\")\n",
        "\n",
        "# (Opcional) Undersampling de negativos no TREINO para acelerar e melhorar aprendizado inicial\n",
        "X_train_seq_bal, y_train_seq_bal = undersample_negatives(\n",
        "    X_train_seq, y_train_seq, max_ratio=NEGATIVE_TO_POS_RATIO_TRAIN, seed=SEED\n",
        ")\n",
        "print(f\"Após undersampling  -> X_train: {X_train_seq_bal.shape}, positivos: {y_train_seq_bal.sum()}, \"\n",
        "      f\"negativos: {len(y_train_seq_bal) - y_train_seq_bal.sum()}\")\n",
        "\n",
        "# Pesos de classe (baseados no conjunto de treino balanceado/atual)\n",
        "classes = np.array([0, 1])\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=classes,\n",
        "    y=y_train_seq_bal\n",
        ")\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(f\"Class weights: {class_weight_dict}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2V0WQ41OOJR",
        "outputId": "3d02862c-930c-41b2-f740-e822d5a03c31"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================================\n",
            "Preparação dos dados para LSTM (split temporal, escalonamento, janelas)\n",
            "=======================================================================\n",
            "Train+Val shape: (227845, 31) | Test shape: (56962, 31)\n",
            "Train shape: (205060, 31) | Val shape: (22785, 31) | Test shape: (56962, 31)\n",
            "Seq shapes -> X_train: (205050, 10, 30), X_val: (22775, 10, 30), X_test: (56952, 10, 30)\n",
            "Após undersampling  -> X_train: (4323, 10, 30), positivos: 393, negativos: 3930\n",
            "Class weights: {0: np.float64(0.55), 1: np.float64(5.5)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo LSTM"
      ],
      "metadata": {
        "id": "tmQ1uCmHPV4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_header(\"Construindo e treinando o modelo LSTM\")\n",
        "\n",
        "n_features = X_train_seq.shape[-1]\n",
        "model = build_lstm_model(n_features=n_features, lookback=LOOKBACK, lr=LEARNING_RATE)\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_seq_bal, y_train_seq_bal,\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Curvas de aprendizado (loss e AUC)\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "ax.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "ax.set_title(\"Curva de aprendizado - Loss\")\n",
        "ax.set_xlabel(\"Época\"); ax.set_ylabel(\"Loss\"); ax.legend()\n",
        "plt.close(fig)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.plot(history.history[\"auc\"], label=\"train_auc\")\n",
        "ax.plot(history.history[\"val_auc\"], label=\"val_auc\")\n",
        "ax.set_title(\"Curva de aprendizado - AUC\")\n",
        "ax.set_xlabel(\"Época\"); ax.set_ylabel(\"AUC\"); ax.legend()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VBOPIMjFOmtx",
        "outputId": "0e16d9cc-495b-4eea-90de-c9ae3e0b891e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====================================\n",
            "Construindo e treinando o modelo LSTM\n",
            "=====================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m24,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,537\u001b[0m (146.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,537</span> (146.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,409\u001b[0m (146.13 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,409</span> (146.13 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "9/9 - 7s - 783ms/step - auc: 0.5761 - loss: 0.6879 - precision: 0.0973 - recall: 0.8321 - val_auc: 0.5248 - val_loss: 0.6515 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 2/30\n",
            "9/9 - 2s - 226ms/step - auc: 0.6750 - loss: 0.6555 - precision: 0.1186 - recall: 0.7710 - val_auc: 0.5194 - val_loss: 0.5908 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 3/30\n",
            "9/9 - 2s - 248ms/step - auc: 0.7003 - loss: 0.6373 - precision: 0.1431 - recall: 0.7023 - val_auc: 0.5263 - val_loss: 0.5328 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 4/30\n",
            "9/9 - 3s - 358ms/step - auc: 0.7177 - loss: 0.6229 - precision: 0.1622 - recall: 0.6361 - val_auc: 0.5273 - val_loss: 0.4860 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 5/30\n",
            "9/9 - 4s - 407ms/step - auc: 0.7316 - loss: 0.6140 - precision: 0.1731 - recall: 0.6463 - val_auc: 0.5362 - val_loss: 0.4525 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 6/30\n",
            "9/9 - 1s - 161ms/step - auc: 0.7519 - loss: 0.5961 - precision: 0.1879 - recall: 0.6489 - val_auc: 0.5533 - val_loss: 0.4247 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 7/30\n",
            "9/9 - 3s - 316ms/step - auc: 0.7590 - loss: 0.5850 - precision: 0.1956 - recall: 0.6539 - val_auc: 0.5588 - val_loss: 0.3919 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 8/30\n",
            "9/9 - 3s - 310ms/step - auc: 0.7672 - loss: 0.5767 - precision: 0.1922 - recall: 0.6997 - val_auc: 0.5699 - val_loss: 0.3383 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 9/30\n",
            "9/9 - 2s - 231ms/step - auc: 0.7803 - loss: 0.5655 - precision: 0.1899 - recall: 0.7354 - val_auc: 0.5857 - val_loss: 0.3534 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 10/30\n",
            "9/9 - 1s - 165ms/step - auc: 0.7950 - loss: 0.5527 - precision: 0.1958 - recall: 0.7532 - val_auc: 0.5859 - val_loss: 0.3267 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 11/30\n",
            "9/9 - 1s - 163ms/step - auc: 0.8010 - loss: 0.5447 - precision: 0.2035 - recall: 0.7430 - val_auc: 0.5877 - val_loss: 0.2680 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 12/30\n",
            "9/9 - 2s - 198ms/step - auc: 0.8160 - loss: 0.5288 - precision: 0.2065 - recall: 0.7252 - val_auc: 0.5803 - val_loss: 0.2332 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 13/30\n",
            "9/9 - 2s - 195ms/step - auc: 0.8245 - loss: 0.5173 - precision: 0.2213 - recall: 0.7659 - val_auc: 0.5808 - val_loss: 0.2116 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "9/9 - 2s - 197ms/step - auc: 0.8437 - loss: 0.4981 - precision: 0.2396 - recall: 0.7659 - val_auc: 0.5834 - val_loss: 0.2152 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 15/30\n",
            "9/9 - 4s - 440ms/step - auc: 0.8529 - loss: 0.4872 - precision: 0.2563 - recall: 0.7812 - val_auc: 0.5764 - val_loss: 0.1906 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 16/30\n",
            "9/9 - 4s - 407ms/step - auc: 0.8578 - loss: 0.4816 - precision: 0.2544 - recall: 0.7761 - val_auc: 0.5783 - val_loss: 0.1608 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação (threshold otimizado na validação)"
      ],
      "metadata": {
        "id": "h9yYR25rPaXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_header(\"Avaliação no conjunto de teste\")\n",
        "\n",
        "# Probabilidades\n",
        "y_val_scores = model.predict(X_val_seq, batch_size=BATCH_SIZE).ravel()\n",
        "y_test_scores = model.predict(X_test_seq, batch_size=BATCH_SIZE).ravel()\n",
        "\n",
        "# Escolher threshold via F1 na validação\n",
        "best_t, best_f1_val = best_threshold_from_validation(y_val_seq, y_val_scores)\n",
        "print(f\"Threshold ótimo (val) p/ F1: {best_t:.3f} | F1(val)={best_f1_val:.4f}\")\n",
        "\n",
        "# Predições binarias no teste\n",
        "y_test_pred = (y_test_scores >= best_t).astype(int)\n",
        "\n",
        "# Métricas no teste\n",
        "acc  = accuracy_score(y_test_seq, y_test_pred)\n",
        "prec = precision_score(y_test_seq, y_test_pred, zero_division=0)\n",
        "rec  = recall_score(y_test_seq, y_test_pred, zero_division=0)\n",
        "f1   = f1_score(y_test_seq, y_test_pred, zero_division=0)\n",
        "auc  = roc_auc_score(y_test_seq, y_test_scores)\n",
        "\n",
        "print_header(\"Métricas no Teste\")\n",
        "print(f\"Accuracy : {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall   : {rec:.4f}\")\n",
        "print(f\"F1-score : {f1:.4f}\")\n",
        "print(f\"AUC-ROC  : {auc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Teste):\")\n",
        "print(classification_report(y_test_seq, y_test_pred, digits=4, zero_division=0))\n",
        "\n",
        "# Matriz de confusão\n",
        "cm = confusion_matrix(y_test_seq, y_test_pred)\n",
        "fig, ax = plt.subplots(figsize=(4,3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
        "ax.set_title(\"Matriz de Confusão (Teste)\")\n",
        "ax.set_xlabel(\"Predito\"); ax.set_ylabel(\"Real\")\n",
        "plt.close(fig)\n",
        "\n",
        "# Curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test_seq, y_test_scores)\n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "ax.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\n",
        "ax.plot([0,1],[0,1],\"--\", label=\"Aleatório\")\n",
        "ax.set_title(\"Curva ROC (Teste)\")\n",
        "ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\"); ax.legend()\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgG77IqCPe50",
        "outputId": "b5a91901-bb1c-4cc7-9217-a48c8d5a823e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Avaliação no conjunto de teste\n",
            "==============================\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step\n",
            "Threshold ótimo (val) p/ F1: 0.400 | F1(val)=0.0071\n",
            "\n",
            "=================\n",
            "Métricas no Teste\n",
            "=================\n",
            "Accuracy : 0.9926\n",
            "Precision: 0.0028\n",
            "Recall   : 0.0133\n",
            "F1-score : 0.0047\n",
            "AUC-ROC  : 0.5186\n",
            "\n",
            "Classification Report (Teste):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9987    0.9938    0.9963     56877\n",
            "           1     0.0028    0.0133    0.0047        75\n",
            "\n",
            "    accuracy                         0.9926     56952\n",
            "   macro avg     0.5008    0.5036    0.5005     56952\n",
            "weighted avg     0.9974    0.9926    0.9950     56952\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diagnóstico: over/underfitting (heurístico) e discussão"
      ],
      "metadata": {
        "id": "i0GtMqPyPr3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_header(\"Diagnóstico\")\n",
        "\n",
        "train_loss_min = np.min(history.history[\"loss\"])\n",
        "val_loss_min   = np.min(history.history[\"val_loss\"])\n",
        "train_auc_max  = np.max(history.history[\"auc\"])\n",
        "val_auc_max    = np.max(history.history[\"val_auc\"])\n",
        "\n",
        "print(f\"Menor loss - treino: {train_loss_min:.4f} | validação: {val_loss_min:.4f}\")\n",
        "print(f\"Maior AUC  - treino: {train_auc_max:.4f} | validação: {val_auc_max:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94k5UsjzPnmI",
        "outputId": "8ecfc68d-84b0-4e07-dc4e-89a6b2f4e6a7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===========\n",
            "Diagnóstico\n",
            "===========\n",
            "Menor loss - treino: 0.4816 | validação: 0.1608\n",
            "Maior AUC  - treino: 0.8578 | validação: 0.5877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise Geral\n",
        "\n",
        "**EDA**\n",
        "\n",
        "* Sem nulos em nenhuma coluna (284.807 linhas, 31 colunas). 👍\n",
        "* Desbalanceamento extremo: só 0,17% de fraudes (492 casos). Isso explica por que accuracy alto pode enganar.\n",
        "* Amount é altamente assimétrico (máx. 25.691), típico de transações.\n",
        "* Maior associação com fraude (ponto-biserial): V17, V14, V12, V10… todas com correlações moderadas e negativas (ex.: V17 ≈ -0,33). Isso sugere que as combinações PCA capturam padrões úteis, mas não fortíssimos.\n",
        "\n",
        "**Preparação — escolhas e impactos**\n",
        "\n",
        "* Split temporal (train/val/test = 205k/22,8k/56,9k) — correto para simular produção e evitar vazamento futuro→passado.\n",
        "* Escalonamento com StandardScaler apenas no treino — correto.\n",
        "* Janelas LSTM (lookback=10) montadas na sequência global de transações. Sem um ID de entidade (cartão/cliente), a sequência “emenda” usuários diferentes; isso dilui o sinal temporal que a LSTM precisa.\n",
        "* Undersampling do treino para ~10:1 (neg:pos) e class weights (0: 0,55; 1: 5,5).\n",
        "\n",
        "**Treino**\n",
        "\n",
        "* Arquitetura LSTM leve (~38k parâmetros) + BN/Dropout + EarlyStopping/LR scheduler — tecnicamente ok.\n",
        "* AUC (treino) subiu até ~0,86, mas AUC (val) ficou em ~0,59 no melhor ponto (época 11).\n",
        "* * As métricas Keras de val_precision/val_recall zeradas por várias épocas revelam que, no threshold 0,5, o modelo quase não marca positivos na validação (com base no output sigmoide).\n",
        "Val_loss ≪ train_loss não significa “ótimo”: o loss de treino é ponderado por classe, o de validação não — isso sozinho pode deixar o val_loss artificialmente menor.\n",
        "\n",
        "**Avaliação**\n",
        "\n",
        "* Threshold otimizado via F1 na validação: 0,40, mas ainda F1(val) = 0,007 (PR fraca).\n",
        "\n",
        "Teste:\n",
        "\n",
        "* AUC-ROC = 0,519 → praticamente aleatório.\n",
        "* Accuracy = 0,993 (enganoso pelo desbalanceamento).\n",
        "* Precision = 0,0028, Recall = 0,0133, F1 = 0,0047.\n",
        "Em números humanos: de 75 fraudes no teste, o modelo pegou ~1; e entre centenas de alertas, quase todos são falsos.\n",
        "\n",
        "**Diagnóstico**\n",
        "\n",
        "* Gap treino vs validação (AUC 0,86 vs 0,59) sugere overfitting ao treino balanceado/ponderado.\n",
        "* Sinal temporal pobre para LSTM: sem sequências por entidade, a rede aprende “ruído global” em vez de padrões de comportamento do mesmo cartão/usuário.\n",
        "* Métrica inadequada durante o treino: accuracy/precision/recall com threshold fixo de 0,5 pouco ajudaram; AUC até ajuda, mas em classes raras PR-AUC é mais informativa.\n",
        "* Undersampling + class weights no treino, mas val/test reais extremamente desbalanceados → calibração ruim e degradação de PR.\n",
        "\n",
        "**O que eu mudaria (possíveis próximos passos)**\n",
        "\n",
        "1. Se houver ID de entidade, reconstruir o problema como sequências por cartão/cliente, respeitando fronteiras entre entidades nos splits e nas janelas. Isso muda o jogo para LSTM/GRU.\n",
        "2. Baseline forte tabular (sem sequência): LogisticRegression/XGBoost com features simples (Amount log, horário do dia, idade da conta, contagens/estatísticas rolling por entidade, interações básicas). Em geral, no Credit 3. 3. Card Fraud clássico, modelos tabulares performam muito bem.\n",
        "Métricas focadas em raridade: acompanhar PR-AUC, Recall@K e otimização de threshold por custo (FN vs FP).\n",
        "4. Reamostrar com cautela: manter val/test com a distribuição real; no treino, testar focal loss ou apenas class weights (sem undersampling agressivo) para melhorar a calibração.\n",
        "5. Engenharia de atributos temporais (se houver entidade): deltas entre transações, contagens no último T minutos, média/mediana/quantis por janela, frequência por comerciante, etc.\n",
        "6. Regularização & simplicidade: se insistir em LSTM, reduzir camadas/neurônios ou aumentar Dropout; e aumentar o lookback só faz sentido se a sequência por mesma entidade existir.\n",
        "7. Calibração de probabilidades (Platt/Isotonic) após o treino para thresholds estáveis.\n"
      ],
      "metadata": {
        "id": "u_A_azSWRwDd"
      }
    }
  ]
}